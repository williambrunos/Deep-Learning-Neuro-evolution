{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williambrunos/Deep-Learning-Neuro-evolution/blob/main/Fundamentals/evolving_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGLnYUqM3NJn"
      },
      "source": [
        "# Evolving Neural Networks\n",
        "\n",
        "## Abstract\n",
        "\n",
        "Evlutionary algorithms are way used to perform topologies, weights, biases and neurons evolutions across neural networks. Sometimes, it's way more pragmatic than other ML algorithms and have to be set to some specific problem.\n",
        "\n",
        "## Algorithm\n",
        "\n",
        "Evolutionary algorithms are based on the premisse of **natural selection**, based on the following steps:\n",
        "\n",
        "- The algorithm starts a population with a certain number of individuals/genomes, with their genotypes or carachteristics being initiated randomly.\n",
        "- The organisms are evaluated based on a **fitness function** or **fitness score**.\n",
        "- The best individuals prom the population are choiced to be parents, performing **asexual reprodution** if the offspring has the same genotype as the parents, or **sexual reprodution** if the offspring has the genotype being a combination of the genotypes of the parents.\n",
        "- Mutate the offspring.\n",
        "- Take back to step two and iterates over these steps until some condition is met, being a maximun number of steps, a threshold of the fitness score etc.\n",
        "\n",
        "## Step 1: The Algorithm\n",
        "\n",
        "We'll be using dense (fully-connected) feed-foward neural networks, like in figure 1:\n",
        "\n",
        "![NN image](https://miro.medium.com/max/700/1*Thii4-1bSrJ1yXwDyW3sjw.png\n",
        ")\n",
        "\n",
        "<center>Dense NN with dimensions [1, 12, 12, 12, 1]</center>\n",
        "\n",
        "In designing our organisms, we have four guiding principles:\n",
        "\n",
        "- We must have control over the input and output dimensions of the organisms. Fundamentally, we are trying to evolve some function f that maps ℝᵃ ⟶ ℝᵇ, so a and b should be built into the organism. We accomplish this by parameterizing the input and output dimensions.\n",
        "- We must have control over the output activation function. The output of the organisms should be appropriate for the problem at hand. We accomplish this by parameterizing the output activation.\n",
        "- We must have control over the complexity of the organisms. The ideal organism should be just complex enough to evolve the target function and no more. We accomplish this by parameterizing the number of hidden layers and their dimensions. In a more advanced algorithm, this could be achieved by letting the organism evolve its own architecture and penalizing complexity with the fitness function.\n",
        "- Organisms must be compatible for sexual reproduction. Fortunately, the above principles ensure that this will be the case. All organisms will have the same architecture, so “exchanging genetic material” here means offspring will get some layer weights from momma and some from poppa.\n",
        "\n",
        "Here's the implementation of an organism:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "DzHmueES6pSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Organism():\n",
        "  def __init__(self, dimensions: list, use_bias=True, output='softmax'):\n",
        "    \"\"\"\n",
        "    This function sets up the neural network, using ReLU as the\n",
        "    activation function for all the hidden layers.\n",
        "\n",
        "    params:\n",
        "    -------\n",
        "    dimensions: list -> list of dimensions of the neural network, being the first\n",
        "    one the dimension of the input and the last one the one from output.\n",
        "    use_bias: bool -> boolean argument indicating if the neurons are going to\n",
        "    have a bias number\n",
        "    output: string -> used to identify the activation function of output\n",
        "    \"\"\"\n",
        "    # layers sets up a matrix of weights for the dimensions\n",
        "    self.layers = []\n",
        "    # a column os biases for each layer of the NN\n",
        "    self.biases = []\n",
        "    # parameter to decide whether to use bias or not\n",
        "    self.use_bias = use_bias\n",
        "    # output is a ativation function\n",
        "    self.output = self._activation(output)\n",
        "    for i in range(len(dimensions)-1):\n",
        "        shape = (dimensions[i], dimensions[i+1])\n",
        "        std = np.sqrt(2 / sum(shape))\n",
        "        layer = np.random.normal(0, std, shape)\n",
        "        bias = np.random.normal(0, std, (1,  dimensions[i+1])) * use_bias\n",
        "        self.layers.append(layer)\n",
        "        self.biases.append(bias)\n",
        "\n",
        "  def _activation(self, output: str) -> function:\n",
        "    \"\"\"\n",
        "    This funcion must return a new activation function to the\n",
        "    output layer of the neural network.\n",
        "\n",
        "    params:\n",
        "    -------\n",
        "    output: str -> this must be a string with the name of the\n",
        "    activation function\n",
        "\n",
        "    returns:\n",
        "    --------\n",
        "    lambda function: function -> represents the activation function by itself\n",
        "    \"\"\"\n",
        "    if output == 'softmax':\n",
        "        return lambda X : np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1, 1)\n",
        "    if output == 'sigmoid':\n",
        "        return lambda X : (1 / (1 + np.exp(-X)))\n",
        "    if output == 'linear':\n",
        "        return lambda X : X\n",
        "\n",
        "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    The predict method repeatedly applies ReLU and matrix multiplication \n",
        "    to the input matrix.\n",
        "\n",
        "    params:\n",
        "    -------\n",
        "    X: matrix input\n",
        "    \"\"\"\n",
        "    if not X.ndim == 2:\n",
        "        raise ValueError(f'Input has {X.ndim} dimensions, expected 2')\n",
        "    if not X.shape[1] == self.layers[0].shape[0]:\n",
        "        raise ValueError(f'Input has {X.shape[1]} features, expected {self.layers[0].shape[0]}')\n",
        "    for index, (layer, bias) in enumerate(zip(self.layers, self.biases)):\n",
        "        X = X @ layer + np.ones((X.shape[0], 1)) @ bias\n",
        "        if index == len(self.layers) - 1:\n",
        "            X = self.output(X) # output activation\n",
        "        else:\n",
        "            X = np.clip(X, 0, np.inf)  # ReLU\n",
        "    return X"
      ],
      "metadata": {
        "id": "1QwEyrAr6S4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Organism Fitness\n",
        "\n",
        "For the time being, it suffices to understand that we require a function scoring_function which accepts an organism as input and returns a real number output, where bigger is better.\n",
        "\n",
        "## Step 3: Reproduction\n",
        "\n",
        "The algorith must select $k$ parents at each iteration: if it this a assexual reproduction, then $k=1$, or $k >= 2$ otherwise. The offspring is set to has $n$ organisms, so the algorithm needs to take $n$ sets of $k$ organisms to compose the parents from the previous generation, considering here a sexual reproduction (and for the following activities).\n",
        "\n",
        "Deciding wich organisms are going to be chosen to compose the set of parents is the initial step of the fitness score, because all the decisions are based on the fitness score of the individuals. Sometimes, depending of the algorithm, it can take a larger amount of genes from the organisms with highest fitness score to compose the genotype of the prole than the ones with lowest scores, but this depends of the implementation of the algorithm. There are some ways to do that:\n",
        "\n",
        "1. Select each parent uniformly from the top 10% of organisms.\n",
        "2. Order the organisms from best to worst, then select the index of each parent by sampling from the exponential distribution.\n",
        "3. Apply the softmax function to each organism’s score to create a probability of selection for each organism, then sample from that distribution.\n",
        "\n",
        "I chose a compromise between methods one and two, where the top 10% of organisms were selected as the first parent of a child ten times each and the second parent was chosen randomly using the exponential distribution, as above. I also enforced that a clone of the best-performing organism of a given generation be included in the next generation, to avoid losing the fitness score in case of making the prole worst than the previous generation. \n",
        "\n",
        "Once n pairs of parents have been chosen, the progeny can be created by randomly combining traits from each pair. In our case, those traits are weights in the neural network layers. Here is the Organism class’s method for progeny creation.\n",
        "\n",
        "Here's some relevant code:"
      ],
      "metadata": {
        "id": "jlBC8OD594xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ecosystem():\n",
        "  # [Some code removed here]\n",
        "  def mate(self, other, mutate=True):\n",
        "    if self.use_bias != other.use_bias:\n",
        "      raise ValueError('Both parents must use bias or not use bias')\n",
        "    if not len(self.layers) == len(other.layers):\n",
        "      raise ValueError('Both parents must have same number of layers')\n",
        "    if not all(self.layers[x].shape == other.layers[x].shape for x in range(len(self.layers))):\n",
        "      raise ValueError('Both parents must have same shape')\n",
        "\n",
        "    child = copy.deepcopy(self)\n",
        "    for i in range(len(child.layers)):\n",
        "      pass_on = np.random.rand(1, child.layers[i].shape[1]) < 0.5\n",
        "      child.layers[i] = pass_on * self.layers[i] + ~pass_on * other.layers[i]\n",
        "      child.biases[i] = pass_on * self.biases[i] + ~pass_on * other.biases[i]\n",
        "    if mutate:\n",
        "      child.mutate()\n",
        "    return child\n",
        "    \n",
        "  def generation(self, repeats=1, keep_best=True):\n",
        "    # Scores each individual 'x' on the population 'repeats' times and takes the mean of it\n",
        "    rewards = [np.mean([self.scoring_function(x) for _ in range(repeats)]) for x in self.population]\n",
        "    # Sorts the individuals based on their rewards (ascending order)\n",
        "    self.population = [self.population[x] for x in np.argsort(rewards)[::-1]]\n",
        "    # Offspring population\n",
        "    new_population = []\n",
        "    for i in range(self.population_size):\n",
        "      # Parent one is selected from the top 10% (holdout is the number\n",
        "      # of organisms that are guaranteed progeny, here being population_size//10)\n",
        "      parent_1_idx = i % self.holdout\n",
        "      if self.mating:\n",
        "        # When mating is True, the second parent is chosen based on the\n",
        "        # exponential distribution\n",
        "        parent_2_idx = min(self.population_size - 1, int(np.random.exponential(self.holdout)))\n",
        "      else:\n",
        "        # When mating is False, the parent 1 = parent 2 and the child is a clone\n",
        "        parent_2_idx = parent_1_idx\n",
        "      offspring = self.population[parent_1_idx].mate(self.population[parent_2_idx])\n",
        "      new_population.append(offspring)\n",
        "    if keep_best:\n",
        "      new_population[-1] = self.population[0] # Ensure best organism survives\n",
        "    self.population = new_population"
      ],
      "metadata": {
        "id": "ZIj4JjOhAnHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refferences\n",
        "\n",
        "[Mediun article - Evolving Neural Networks](https://towardsdatascience.com/evolving-neural-networks-b24517bb3701)\n",
        "\n",
        "[Paper randomly initiating weights](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
        "\n",
        "[Guia markdown](https://medium.com/walternascimentobarroso-pt/curso-r%C3%A1pido-de-markdown-4af49e3bfa65#:~:text=Para%20centralizar%20itens%20no%20markdown,usar%20a%20tag%20.)\n",
        "\n",
        "[Activation functions article](https://usernamejack.medium.com/analyzing-the-activation-functions-of-common-neural-networks-4dcdaa92a055)"
      ],
      "metadata": {
        "id": "CakEXWZX3t7w"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "evolving_neural_networks.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}